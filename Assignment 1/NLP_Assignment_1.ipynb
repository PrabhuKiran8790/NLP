{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Assignment-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment 1\n",
        "\n",
        "\n",
        "<code>\n",
        "20K45A0215 | PRABHU KIRAN KONDA | <a href=\"https://github.com/prabhukiran8790\"> GitHub.com/PrabhuKiran8790 </a>\n",
        "</code>\n",
        "\n",
        "1. split given paragraph to sentences\n",
        "2. split given paragraph to words\n",
        "3. Find stem and lemma words in given paragraph"
      ],
      "metadata": {
        "id": "kmc_5e4mpDIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "eVKEhDa4pbiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB1LSfBaPaKL",
        "outputId": "a1f09569-74d2-4efa-97ef-fa7188bb0fbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. paragraph to sentences\n",
        "\n",
        "def convert_to_sentences(data):\n",
        "    sentences = nltk.sent_tokenize(open(data).read())\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].lower()\n",
        "        sentences[i] = re.sub(r'\\W', ' ', sentences[i])\n",
        "        sentences[i] = re.sub(r'\\s+', ' ', sentences[i])\n",
        "        sentences[i] = sentences[i][:-1]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# 1. paragraph to words\n",
        "\n",
        "def convert_to_words(data):\n",
        "    res = []\n",
        "    for sentence in data:\n",
        "        res.append(sentence.split())\n",
        "    return res"
      ],
      "metadata": {
        "id": "od49nfpOITY7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = convert_to_sentences('data.txt')\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5MDjfSMKEDY",
        "outputId": "dbe42005-9648-4a23-ca49-df8172836dcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['are you fascinated by the amount of text data available on the internet',\n",
              " 'are you looking for ways to work with this text data but aren t sure where to begin',\n",
              " 'machines after all recognize numbers not the letters of our language',\n",
              " 'and that can be a tricky landscape to navigate in machine learning',\n",
              " 'solving an nlp problem is a multi stage process',\n",
              " 'we need to clean the unstructured text data first before we can even think about getting to the modelling stage',\n",
              " 'cleaning the data consists of a few key steps']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_words(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VBisEUQL4Ct",
        "outputId": "a93ead4d-2430-4e28-9d94-3a742c2ed8cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['are',\n",
              "  'you',\n",
              "  'fascinated',\n",
              "  'by',\n",
              "  'the',\n",
              "  'amount',\n",
              "  'of',\n",
              "  'text',\n",
              "  'data',\n",
              "  'available',\n",
              "  'on',\n",
              "  'the',\n",
              "  'internet'],\n",
              " ['are',\n",
              "  'you',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'ways',\n",
              "  'to',\n",
              "  'work',\n",
              "  'with',\n",
              "  'this',\n",
              "  'text',\n",
              "  'data',\n",
              "  'but',\n",
              "  'aren',\n",
              "  't',\n",
              "  'sure',\n",
              "  'where',\n",
              "  'to',\n",
              "  'begin'],\n",
              " ['machines',\n",
              "  'after',\n",
              "  'all',\n",
              "  'recognize',\n",
              "  'numbers',\n",
              "  'not',\n",
              "  'the',\n",
              "  'letters',\n",
              "  'of',\n",
              "  'our',\n",
              "  'language'],\n",
              " ['and',\n",
              "  'that',\n",
              "  'can',\n",
              "  'be',\n",
              "  'a',\n",
              "  'tricky',\n",
              "  'landscape',\n",
              "  'to',\n",
              "  'navigate',\n",
              "  'in',\n",
              "  'machine',\n",
              "  'learning'],\n",
              " ['solving', 'an', 'nlp', 'problem', 'is', 'a', 'multi', 'stage', 'process'],\n",
              " ['we',\n",
              "  'need',\n",
              "  'to',\n",
              "  'clean',\n",
              "  'the',\n",
              "  'unstructured',\n",
              "  'text',\n",
              "  'data',\n",
              "  'first',\n",
              "  'before',\n",
              "  'we',\n",
              "  'can',\n",
              "  'even',\n",
              "  'think',\n",
              "  'about',\n",
              "  'getting',\n",
              "  'to',\n",
              "  'the',\n",
              "  'modelling',\n",
              "  'stage'],\n",
              " ['cleaning', 'the', 'data', 'consists', 'of', 'a', 'few', 'key', 'steps']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "sentences = convert_to_sentences('data2.txt')\n",
        "res = []\n",
        "for e in sentences:\n",
        "    tokenization = nltk.word_tokenize(e)\n",
        "    temp = []\n",
        "    for w in tokenization:\n",
        "        temp.append(wordnet_lemmatizer.lemmatize(w))\n",
        "        # print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n",
        "    res.append(temp)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVUNhlO2jIYC",
        "outputId": "9071dbf7-5447-4272-f5de-ae7943620a11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['text',\n",
              "  'summarization',\n",
              "  'is',\n",
              "  'one',\n",
              "  'of',\n",
              "  'those',\n",
              "  'application',\n",
              "  'of',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'nlp',\n",
              "  'which',\n",
              "  'is',\n",
              "  'bound',\n",
              "  'to',\n",
              "  'have',\n",
              "  'a',\n",
              "  'huge',\n",
              "  'impact',\n",
              "  'on',\n",
              "  'our',\n",
              "  'life'],\n",
              " ['with',\n",
              "  'growing',\n",
              "  'digital',\n",
              "  'medium',\n",
              "  'and',\n",
              "  'ever',\n",
              "  'growing',\n",
              "  'publishing',\n",
              "  'who',\n",
              "  'ha',\n",
              "  'the',\n",
              "  'time',\n",
              "  'to',\n",
              "  'go',\n",
              "  'through',\n",
              "  'entire',\n",
              "  'article',\n",
              "  'document',\n",
              "  'book',\n",
              "  'to',\n",
              "  'decide',\n",
              "  'whether',\n",
              "  'they',\n",
              "  'are',\n",
              "  'useful',\n",
              "  'or',\n",
              "  'not'],\n",
              " ['thankfully', 'this', 'technology', 'is', 'already', 'her']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_list = convert_to_words(sentences)\n",
        "ps = PorterStemmer()\n",
        "res = []\n",
        "for e in words_list:\n",
        "    temp = []\n",
        "    for word in e:\n",
        "        rootWord = ps.stem(word)\n",
        "        temp.append(rootWord)\n",
        "    res.append(temp)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOssyxzNjgzh",
        "outputId": "d01f3fe7-a34f-4cb3-b6fb-8499c02f63c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['text',\n",
              "  'summar',\n",
              "  'is',\n",
              "  'one',\n",
              "  'of',\n",
              "  'those',\n",
              "  'applic',\n",
              "  'of',\n",
              "  'natur',\n",
              "  'languag',\n",
              "  'process',\n",
              "  'nlp',\n",
              "  'which',\n",
              "  'is',\n",
              "  'bound',\n",
              "  'to',\n",
              "  'have',\n",
              "  'a',\n",
              "  'huge',\n",
              "  'impact',\n",
              "  'on',\n",
              "  'our',\n",
              "  'live'],\n",
              " ['with',\n",
              "  'grow',\n",
              "  'digit',\n",
              "  'media',\n",
              "  'and',\n",
              "  'ever',\n",
              "  'grow',\n",
              "  'publish',\n",
              "  'who',\n",
              "  'ha',\n",
              "  'the',\n",
              "  'time',\n",
              "  'to',\n",
              "  'go',\n",
              "  'through',\n",
              "  'entir',\n",
              "  'articl',\n",
              "  'document',\n",
              "  'book',\n",
              "  'to',\n",
              "  'decid',\n",
              "  'whether',\n",
              "  'they',\n",
              "  'are',\n",
              "  'use',\n",
              "  'or',\n",
              "  'not'],\n",
              " ['thank', 'thi', 'technolog', 'is', 'alreadi', 'her']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}